{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from loss import SSIM, CE, MSE, MAE, TV, PSNR, L1, L2, dice_coef_loss, binary_iou\n",
    "from psf import PSFLayer, ZernPSF\n",
    "import pix2pix\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Segmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h = 128\n",
    "img_w = 128\n",
    "\n",
    "\n",
    "def shuffling(x, y):\n",
    "    x, y = shuffle(x, y, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "def load_data(path):\n",
    "    x = sorted(glob(os.path.join(path, \"image\", \"*png\")))\n",
    "    y = sorted(glob(os.path.join(path, \"mask\", \"*png\")))\n",
    "    return x, y\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([img_h, img_w, 3])\n",
    "    y.set_shape([img_h, img_w, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch=2):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(image, mask, y_pred, save_image_path):\n",
    "    line = np.ones((128, 10, 3)) * 128\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=-1)  \n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1) \n",
    "    mask = mask * 255\n",
    "\n",
    "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
    "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)\n",
    "\n",
    "    masked_image = image * y_pred\n",
    "    y_pred = y_pred * 255\n",
    "\n",
    "    cat_images = np.concatenate([image, line, mask, line, y_pred, line, masked_image], axis=1)\n",
    "    cv2.imwrite(save_image_path, cat_images)\n",
    "\n",
    "def save_results_psf(image, mask, x_psf, y_pred, save_image_path):\n",
    "    line = np.ones((128, 10, 3)) * 128\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=-1)  \n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1) \n",
    "    mask = mask * 255\n",
    "\n",
    "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
    "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)\n",
    "\n",
    "    masked_image = image * y_pred\n",
    "    y_pred = y_pred * 255\n",
    "\n",
    "    cat_images = np.concatenate([image, line, mask, line, (x_psf*255), line, y_pred, line, masked_image], axis=1)\n",
    "    cv2.imwrite(save_image_path, cat_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training (without Physical Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',\n",
    "    'block_3_expand_relu',\n",
    "    'block_6_expand_relu',\n",
    "    'block_13_expand_relu',\n",
    "    'block_16_project',   \n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False\n",
    "\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),\n",
    "    pix2pix.upsample(256, 3),\n",
    "    pix2pix.upsample(128, 3),\n",
    "    pix2pix.upsample(64, 3), \n",
    "]\n",
    "\n",
    "def unet_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same', activation='sigmoid')\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 2\n",
    "\n",
    "dataset_path = \"datasets/segmentation\"\n",
    "train_path = os.path.join(dataset_path, \"train\")\n",
    "valid_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "train_x, train_y = load_data(train_path)\n",
    "train_x, train_y = shuffling(train_x, train_y)\n",
    "valid_x, valid_y = load_data(valid_path)\n",
    "\n",
    "train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
    "\n",
    "train_dataset = train_dataset.take(len(train_x))\n",
    "valid_dataset = valid_dataset.take(len(valid_x))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)*batch_size}\")\n",
    "print(f\"Validation size: {len(valid_dataset)*batch_size}\")\n",
    "\n",
    "unet = unet_model(1)\n",
    "\n",
    "MeanIou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "\n",
    "unet.compile(optimizer='adam',\n",
    "                loss=dice_coef_loss,\n",
    "                metrics=binary_iou)\n",
    "unet.fit(train_dataset, epochs=num_epochs, validation_data=valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.save(\"checkpoints/unet_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save plots ##\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "save_path = \"results/segmentation_results/segmentation_without_psf\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "dataset_path = \"datasets/segmentation\"\n",
    "\n",
    "valid_path = os.path.join(dataset_path, \"test\")\n",
    "test_x, test_y = load_data(valid_path)\n",
    "\n",
    "print(\"Test size: \", len(test_x))\n",
    "\n",
    "for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "    img_name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = image/255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # x_psf = unet.layers[1](x)\n",
    "    # x_psf = np.squeeze(x_psf, axis=0)\n",
    "    y_pred = unet.predict(x)[0]\n",
    "    y_pred = np.squeeze(y_pred, axis=-1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "    save_image_path = \"{}/{}.png\".format(save_path, img_name)\n",
    "    # save_results(image, mask, x_psf, y_pred, save_image_path)\n",
    "    save_results(image, mask, y_pred, save_image_path)\n",
    "    if i > 70:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Zernike PSF Layer to the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',\n",
    "    'block_3_expand_relu',\n",
    "    'block_6_expand_relu',\n",
    "    'block_13_expand_relu',\n",
    "    'block_16_project',   \n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "nmodes = 6\n",
    "rad = 5\n",
    "modestart = 1\n",
    "n_channels = 3\n",
    "\n",
    "psf_layer = ZernPSF(nmodes, rad, modestart, n_channels, False)\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),\n",
    "    pix2pix.upsample(256, 3),\n",
    "    pix2pix.upsample(128, 3),\n",
    "    pix2pix.upsample(64, 3), \n",
    "]\n",
    "\n",
    "def unet_psf_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "  x_psf = psf_layer(inputs)\n",
    "\n",
    "  skips = down_stack(x_psf)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same', activation='sigmoid')\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "unet_psf_mse = unet_psf_model(1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = unet_psf_mse(x, training=True)\n",
    "        x_psf = unet_psf_mse.layers[1](x)\n",
    "        loss_value = dice_coef_loss(y, output) - MSE(x_psf, x)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, unet_psf_mse.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, unet_psf_mse.trainable_weights))\n",
    "\n",
    "    train_iou = binary_iou(y, output)\n",
    "\n",
    "    return loss_value, train_iou\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    train_iou_ls = []\n",
    "    val_iou_ls = []\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value, train_iou = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Training loss at step {} = {}\".format(step, np.round(loss_value, 5)))\n",
    "            print(\"Training IoU at step {} = {}\".format(step, np.round(train_iou, 5)))\n",
    "        train_iou_ls.append(train_iou)\n",
    "\n",
    "    print(\"Train IoU: {}\".format(np.round(tf.reduce_mean(train_iou_ls), 5)))\n",
    "\n",
    "\n",
    "    for x_batch_val, y_batch_val in valid_dataset:\n",
    "        val_output = unet_psf_mse(x_batch_val, training=False)\n",
    "        val_iou_ls.append(binary_iou(y_batch_val, val_output))\n",
    "\n",
    "    print(\"Validation IoU: {}\".format(np.round(tf.reduce_mean(val_iou_ls), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate privacy using SSIM, MS-SSIM, and PSNR\n",
    "ssim_loss = []\n",
    "ms_ssim_loss = []\n",
    "psnr_loss = []\n",
    "for step, (x_batch_test, y_batch_test) in enumerate(valid_dataset):\n",
    "    x_psf_pred = unet_psf_mse.layers[1](x_batch_test, training=False)\n",
    "    #rescale to [0, 1]\n",
    "    for idx in range(x_psf_pred.shape[0]):\n",
    "        ssim_loss.append(SSIM(x_batch_test[idx], x_psf_pred[idx]))\n",
    "        ms_ssim_loss.append(tf.image.ssim_multiscale(x_batch_test[idx], x_psf_pred[idx], max_val=1.0, filter_size=8))\n",
    "        psnr_loss.append(PSNR(x_batch_test[idx], x_psf_pred[idx]))\n",
    "\n",
    "print('SSIM Loss: ', np.mean(ssim_loss))\n",
    "print('MS-SSIM Loss: ', np.mean(ms_ssim_loss))\n",
    "print('PSNR Loss: ', np.mean(psnr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save plots ##\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "save_path = \"results/segmentation_results/segmentation_with_psf\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "dataset_path = \"datasets/segmentation\"\n",
    "\n",
    "valid_path = os.path.join(dataset_path, \"test\")\n",
    "test_x, test_y = load_data(valid_path)\n",
    "\n",
    "print(\"Test size: \", len(test_x))\n",
    "\n",
    "for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "    img_name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = image/255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    x_psf = unet_psf_mse.layers[1](x)\n",
    "    x_psf = np.squeeze(x_psf, axis=0)\n",
    "    y_pred = unet_psf_mse.predict(x)[0]\n",
    "    y_pred = np.squeeze(y_pred, axis=-1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "    save_image_path = \"{}/{}.png\".format(save_path, img_name)\n",
    "    save_results_psf(image, mask, x_psf, y_pred, save_image_path)\n",
    "    # save_results(image, mask, y_pred, save_image_path)\n",
    "    if i > 70:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',\n",
    "    'block_3_expand_relu',\n",
    "    'block_6_expand_relu',\n",
    "    'block_13_expand_relu',\n",
    "    'block_16_project',   \n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False\n",
    "\n",
    "\n",
    "nmodes = 6\n",
    "rad = 5\n",
    "modestart = 1\n",
    "n_channels = 3\n",
    "\n",
    "psf_layer = ZernPSF(nmodes, rad, modestart, n_channels, True)\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),\n",
    "    pix2pix.upsample(256, 3),\n",
    "    pix2pix.upsample(128, 3),\n",
    "    pix2pix.upsample(64, 3), \n",
    "]\n",
    "\n",
    "def unet_psf_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "  x_psf = psf_layer(inputs)\n",
    "\n",
    "  skips = down_stack(x_psf)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same', activation='sigmoid')\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "unet_psf = unet_psf_model(1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "\n",
    "def gradient_loss(x, x_psf):\n",
    "    x_grad = tf.image.sobel_edges(x)\n",
    "    x_psf_grad = tf.image.sobel_edges(x_psf)\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(x_grad, x_psf_grad))\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = unet_psf(x, training=True)\n",
    "        x_psf = unet_psf.layers[1](x)\n",
    "        loss_value = dice_coef_loss(y, output) - MSE(x_psf, x) + gradient_loss(x, x_psf)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, unet_psf.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, unet_psf.trainable_weights))\n",
    "\n",
    "    train_iou = binary_iou(y, output)\n",
    "\n",
    "    return loss_value, train_iou\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    train_iou = []\n",
    "    val_iou = []\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value, train_iou = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Training loss at step {} = {}\".format(step, np.round(loss_value, 5)))\n",
    "            print(\"Training IoU at step {} = {}\".format(step, np.round(train_iou, 5)))\n",
    "        train_iou.append(train_iou)\n",
    "\n",
    "    print(\"Train IoU: {}\".format(np.round(tf.reduce_mean(train_iou), 5)))\n",
    "\n",
    "\n",
    "    for x_batch_val, y_batch_val in valid_dataset:\n",
    "        val_output = unet_psf(x_batch_val, training=False)\n",
    "        val_iou.append(binary_iou(y_batch_val, val_output))\n",
    "\n",
    "\n",
    "    print(\"Validation IoU: {}\".format(np.round(tf.reduce_mean(val_iou), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iou = []\n",
    "for x_batch_val, y_batch_val in valid_dataset:\n",
    "    val_output = unet_psf(x_batch_val, training=False)\n",
    "    val_iou.append(binary_iou(y_batch_val, val_output))\n",
    "tf.reduce_mean(val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save plots ##\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "save_path = \"results/segmentation_results/segmentation_with_psf\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "dataset_path = \"datasets/segmentation\"\n",
    "\n",
    "valid_path = os.path.join(dataset_path, \"test\")\n",
    "test_x, test_y = load_data(valid_path)\n",
    "\n",
    "print(\"Test size: \", len(test_x))\n",
    "\n",
    "for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "    img_name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = image/255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    x_psf = unet_psf.layers[1](x)\n",
    "    x_psf = np.squeeze(x_psf, axis=0)\n",
    "    y_pred = unet_psf.predict(x)[0]\n",
    "    y_pred = np.squeeze(y_pred, axis=-1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "    save_image_path = \"{}/{}.png\".format(save_path, img_name)\n",
    "    save_results_psf(image, mask, x_psf, y_pred, save_image_path)\n",
    "    # save_results(image, mask, y_pred, save_image_path)\n",
    "    if i > 70:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate privacy using SSIM, MS-SSIM, and PSNR\n",
    "ssim_loss = []\n",
    "ms_ssim_loss = []\n",
    "psnr_loss = []\n",
    "for step, (x_batch_test, y_batch_test) in enumerate(valid_dataset):\n",
    "    x_psf_pred = unet_psf.layers[1](x_batch_test, training=False)\n",
    "    for idx in range(x_psf_pred.shape[0]):\n",
    "        ssim_loss.append(SSIM(x_batch_test[idx], x_psf_pred[idx]))\n",
    "        ms_ssim_loss.append(tf.image.ssim_multiscale(x_batch_test[idx], x_psf_pred[idx], max_val=1.0, filter_size=8))\n",
    "        psnr_loss.append(PSNR(x_batch_test[idx], x_psf_pred[idx]))\n",
    "\n",
    "print('SSIM Loss: ', np.mean(ssim_loss))\n",
    "print('MS-SSIM Loss: ', np.mean(ms_ssim_loss))\n",
    "print('PSNR Loss: ', np.mean(psnr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test without Sobel\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "unet_psf_mse = unet_psf_model(1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = unet_psf_mse(x, training=True)\n",
    "        x_psf = unet_psf_mse.layers[1](x)\n",
    "        loss_value = dice_coef_loss(y, output) - MSE(x_psf, x)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, unet_psf_mse.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, unet_psf_mse.trainable_weights))\n",
    "\n",
    "    train_iou = binary_iou(y, output)\n",
    "\n",
    "    return loss_value, train_iou\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    train_iou_ls = []\n",
    "    val_iou_ls = []\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value, train_iou = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Training loss at step {} = {}\".format(step, np.round(loss_value, 5)))\n",
    "            print(\"Training IoU at step {} = {}\".format(step, np.round(train_iou, 5)))\n",
    "        train_iou_ls.append(train_iou)\n",
    "\n",
    "    print(\"Train IoU: {}\".format(np.round(tf.reduce_mean(train_iou_ls), 5)))\n",
    "\n",
    "\n",
    "    for x_batch_val, y_batch_val in valid_dataset:\n",
    "        val_output = unet_psf_mse(x_batch_val, training=False)\n",
    "        val_iou_ls.append(binary_iou(y_batch_val, val_output))\n",
    "\n",
    "    print(\"Validation IoU: {}\".format(np.round(tf.reduce_mean(val_iou_ls), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_psf_mse.save(\"checkpoints/unet_psf_zern_mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iou_ls = []\n",
    "for x_batch_val, y_batch_val in valid_dataset:\n",
    "    val_output = unet_psf_mse(x_batch_val, training=False)\n",
    "    val_iou_ls.append(binary_iou(y_batch_val, val_output))\n",
    "\n",
    "print(\"Validation IoU: {}\".format(np.round(tf.reduce_mean(val_iou_ls), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate privacy using SSIM, MS-SSIM, and PSNR\n",
    "ssim_loss = []\n",
    "ms_ssim_loss = []\n",
    "psnr_loss = []\n",
    "for step, (x_batch_test, y_batch_test) in enumerate(valid_dataset):\n",
    "    x_psf_pred = unet_psf_mse.layers[1](x_batch_test, training=False)\n",
    "    #rescale to [0, 1]\n",
    "    x_psf_pred = (x_psf_pred - tf.reduce_min(x_psf_pred)) / (tf.reduce_max(x_psf_pred) - tf.reduce_min(x_psf_pred))\n",
    "    for idx in range(x_psf_pred.shape[0]):\n",
    "        ssim_loss.append(SSIM(x_batch_test[idx], x_psf_pred[idx]))\n",
    "        ms_ssim_loss.append(tf.image.ssim_multiscale(x_batch_test[idx], x_psf_pred[idx], max_val=1.0, filter_size=8))\n",
    "        psnr_loss.append(PSNR(x_batch_test[idx], x_psf_pred[idx]))\n",
    "\n",
    "print('SSIM Loss: ', np.mean(ssim_loss))\n",
    "print('MS-SSIM Loss: ', np.mean(ms_ssim_loss))\n",
    "print('PSNR Loss: ', np.mean(psnr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save plots ##\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "save_path = \"results/segmentation_results/segmentation_with_psf_without_sobel\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "dataset_path = \"datasets/segmentation\"\n",
    "\n",
    "valid_path = os.path.join(dataset_path, \"test\")\n",
    "test_x, test_y = load_data(valid_path)\n",
    "\n",
    "print(\"Test size: \", len(test_x))\n",
    "\n",
    "for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "    img_name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = image/255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    x_psf = unet_psf_mse.layers[1](x)\n",
    "    x_psf = np.squeeze(x_psf, axis=0)\n",
    "    y_pred = unet_psf_mse.predict(x)[0]\n",
    "    y_pred = np.squeeze(y_pred, axis=-1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "    save_image_path = \"{}/{}.png\".format(save_path, img_name)\n",
    "    save_results_psf(image, mask, x_psf, y_pred, save_image_path)\n",
    "    # save_results(image, mask, y_pred, save_image_path)\n",
    "    if i > 70:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
